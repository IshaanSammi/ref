{"cells":[{"cell_type":"markdown","id":"3295b0a3","metadata":{"id":"3295b0a3"},"source":["## 1. Document ingestion\n","\n","In this step, the notebook loads the paper and prepares it for downstream tasks. The code uses requests to grab the PDF and fitz (PyMuPDF) to walk through each page. Then the text on every page is split into fixed-size chunks so it can be embedded later, and any images on the pages are extracted and saved.\n"]},{"cell_type":"code","execution_count":null,"id":"035c515a","metadata":{"id":"035c515a"},"outputs":[],"source":["!pip install PyMuPDF\n","import os\n","import io\n","import requests\n","import fitz\n","from PIL import Image\n","\n","\n","\n","\n","pdf_url = \"https://arxiv.org/pdf/1706.03762.pdf\"\n","pdf_path = \"attention.pdf\"\n","\n","\n","if not os.path.exists(pdf_path):\n","    response = requests.get(pdf_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n","    if response.status_code == 200:\n","        with open(pdf_path, \"wb\") as f:\n","            f.write(response.content)\n","    else:\n","        raise RuntimeError(f\"Failed to download PDF (status {response.status_code}). Please download it manually and place it at {pdf_path}.\")\n","\n","\n","doc = fitz.open(pdf_path)\n","chunks = []  # list of {page, text}\n","images = []  # list of {page, image}\n","chunk_size = 500\n","for page_num in range(doc.page_count):\n","    page = doc[page_num]\n","    text = page.get_text().strip()\n","\n","    for i in range(0, len(text), chunk_size):\n","        chunk_text = text[i:i + chunk_size]\n","        chunks.append({\"page\": page_num, \"text\": chunk_text})\n","\n","    for img_index, img_info in enumerate(page.get_images(full=True)):\n","        xref = img_info[0]\n","        base_image = doc.extract_image(xref)\n","        image_bytes = base_image[\"image\"]\n","        pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n","        images.append({\"page\": page_num, \"image\": pil_image})\n","\n","doc.close()"]},{"cell_type":"markdown","id":"c59fe34a","metadata":{"id":"c59fe34a"},"source":["## 2. Embedding with Jina CLIP V1\n","\n","For embeddings, the notebook uses Jina CLIP V1, a multimodal model that maps both text and images into a shared vector space so they can be compared directly. The model is loaded via the Hugging Face transformers API with trust_remote_code=True, which lets it expose a higher-level interface on top of the base CLIP architecture.\n"]},{"cell_type":"code","execution_count":null,"id":"D_q_fWZqB8_C","metadata":{"id":"D_q_fWZqB8_C","collapsed":true},"outputs":[],"source":["import torch\n","from transformers import AutoModel\n","import numpy as np\n","from collections import defaultdict\n","\n","\n","clip_model = AutoModel.from_pretrained(\"jinaai/jina-clip-v1\", trust_remote_code=True)\n","clip_model.eval()\n","\n","USE_CUDA = torch.cuda.is_available()\n","USE_FP16 = False  # flip to True only if GPU hits memory limits\n","\n","if USE_CUDA:\n","    clip_model = clip_model.to(\"cuda\")\n","    if USE_FP16:\n","        clip_model = clip_model.half()\n","\n","\n","texts = [c[\"text\"] for c in chunks]\n","image_list = [im[\"image\"] for im in images] if images else []\n","\n","\n","page_to_images = defaultdict(list)\n","for im in images:\n","    page_to_images[im[\"page\"]].append(im[\"image\"])\n","\n","\n","def batched(seq, n):\n","    for i in range(0, len(seq), n):\n","        yield seq[i:i+n]\n","\n","\n","@torch.no_grad()\n","def encode_text(text_list):\n","    # Jina-CLIP remote code returns numpy arrays\n","    return clip_model.encode_text(text_list)\n","\n","@torch.no_grad()\n","def encode_images(pil_images):\n","    return clip_model.encode_image(pil_images)\n","\n","\n","TEXT_BATCH = 64\n","IMG_BATCH = 16\n","\n","text_embeds = []\n","for b in batched(texts, TEXT_BATCH):\n","    if b:\n","        text_embeds.append(encode_text(b))\n","text_embeddings = np.vstack(text_embeds) if text_embeds else np.zeros((0, 768), dtype=np.float32)\n","\n","image_embeds = []\n","for b in batched(image_list, IMG_BATCH):\n","    if b:\n","        image_embeds.append(encode_images(b))\n","image_embeddings = np.vstack(image_embeds) if image_embeds else np.zeros((0, 768), dtype=np.float32)\n","\n","\n","def l2_normalize(a, eps=1e-12):\n","    if a.size == 0:\n","        return a\n","    norms = np.linalg.norm(a, axis=1, keepdims=True)\n","    return a / np.maximum(norms, eps)\n","\n","text_embeddings = l2_normalize(text_embeddings)\n","image_embeddings = l2_normalize(image_embeddings)\n","\n"]},{"cell_type":"markdown","id":"257cab70","metadata":{"id":"257cab70"},"source":["## 3. Storing embeddings in Chroma\n","\n","To persist the embeddings, the notebook uses the Chroma vector database, which is designed for fast similarity search over high‑dimensional vectors. It relies on approximate nearest‑neighbour indexing so that relevant chunks can be retrieved quickly even as the collection grows.\n"]},{"cell_type":"code","execution_count":null,"id":"05fe6aee","metadata":{"id":"05fe6aee","collapsed":true},"outputs":[],"source":["!pip install chromadb\n","import chromadb\n","from math import ceil\n","\n","\n","db_client = chromadb.Client()\n","\n","collection = db_client.get_or_create_collection(name=\"transformer_paper\")\n","\n","\n","texts = [c[\"text\"] for c in chunks]\n","text_ids = [f\"text_{i}\" for i in range(len(texts))]\n","text_metadatas = [{\"page\": c[\"page\"], \"type\": \"text\", \"idx\": i} for i, c in enumerate(chunks)]\n","\n","\n","try:\n","    _ = text_embeddings\n","except NameError:\n","    text_embeddings = encode_text(texts)\n","\n","\n","text_vectors = [row.tolist() for row in text_embeddings]\n","\n","\n","BATCH = 512\n","for b in range(ceil(len(texts) / BATCH)):\n","    s = b * BATCH\n","    e = min((b + 1) * BATCH, len(texts))\n","    if s < e:\n","        collection.add(\n","            ids=text_ids[s:e],\n","            embeddings=text_vectors[s:e],\n","            metadatas=text_metadatas[s:e],\n","            documents=texts[s:e],\n","        )\n","\n","if images:\n","    pil_images = [im[\"image\"] for im in images]\n","    image_ids = [f\"img_{i}\" for i in range(len(images))]\n","    image_metadatas = [{\"page\": im[\"page\"], \"type\": \"image\", \"idx\": i} for i, im in enumerate(images)]\n","\n","\n","    try:\n","        _ = image_embeddings\n","    except NameError:\n","        image_embeddings = encode_images(pil_images)\n","\n","    image_vectors = [row.tolist() for row in image_embeddings]\n","\n","    for b in range(ceil(len(pil_images) / BATCH)):\n","        s = b * BATCH\n","        e = min((b + 1) * BATCH, len(pil_images))\n","        if s < e:\n","            collection.add(\n","                ids=image_ids[s:e],\n","                embeddings=image_vectors[s:e],\n","                metadatas=image_metadatas[s:e],\n","                documents=[\"\"] * (e - s),\n","            )"]},{"cell_type":"code","source":["print(collection.count())"],"metadata":{"id":"xDkZBK_FL5gx","collapsed":true},"id":"xDkZBK_FL5gx","execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(texts)+len(images)"],"metadata":{"id":"9EooNfvXMkjv","collapsed":true},"id":"9EooNfvXMkjv","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"4ef256a2","metadata":{"id":"4ef256a2"},"source":["## 4. Retrieval\n","\n","For retrieval, the notebook queries the Chroma collection by first embedding the query string using Jina CLIP. The resulting embedding is then used to find the top-k nearest neighbours in the vector database.​\n","\n","The retrieval function returns the matching documents, their associated metadata (like page or chunk ID), and the distance scores that indicate how closely each result matches the query. This allows for both fast similarity search and easy tracing back to the original context.\n"]},{"cell_type":"code","execution_count":null,"id":"de693fed","metadata":{"id":"de693fed"},"outputs":[],"source":["import numpy as np\n","\n","def _l2_normalize(v, eps=1e-12):\n","    if v.ndim == 1:\n","        n = np.linalg.norm(v) or eps\n","        return v / max(n, eps)\n","    n = np.linalg.norm(v, axis=1, keepdims=True)\n","    return v / np.maximum(n, eps)\n","\n","def retrieve(query, top_k=3, where=None):\n","    \"\"\"\n","    Retrieve most relevant text chunks for a query.\n","    - Normalizes query embedding to match index normalization.\n","    - Optional `where` filter; defaults to text-only.\n","    \"\"\"\n","    if where is None:\n","        where = {\"type\": \"text\"}\n","\n","\n","    q_emb = encode_text([query])[0]\n","    q_emb = _l2_normalize(q_emb)\n","\n","\n","    count = collection.count()\n","    k = min(top_k, max(count, 0))\n","\n","    if k == 0:\n","        return {\"documents\": [], \"metadatas\": [], \"distances\": []}\n","\n","    res = collection.query(\n","        query_embeddings=[q_emb.tolist()],\n","        n_results=k,\n","        include=[\"documents\", \"metadatas\", \"distances\"],\n","        where=where\n","    )\n","    return res\n","\n","def show_results(res, max_chars=160):\n","    \"\"\"Pretty-print results for quick sanity checks.\"\"\"\n","    if not res or not res.get(\"documents\"):\n","        return\n","    docs = res[\"documents\"][0]\n","    metas = res[\"metadatas\"][0]\n","    dists = res[\"distances\"][0]\n","    for i, (doc, meta, dist) in enumerate(zip(docs, metas, dists), 1):\n","        snippet = (doc[:max_chars] + \"…\") if doc and len(doc) > max_chars else (doc or \"\")\n","        page = meta.get(\"page\", \"?\")\n"]},{"cell_type":"markdown","id":"35c5d336","metadata":{"id":"35c5d336"},"source":["## 5. Generation with Phi-3 Vision\n","\n","The notebook uses the Phi-3 Vision model for multimodal generation, which can process both a single image and a long text prompt in a special chat format. The input should be structured like this:\n","\n","```\n","<|user|>\n","<|image_1|>\n","{prompt}<|end|>\n","<|assistant|>\n","\n","```\n","\n","A small blank image is supplied if no relevant image was retrieved.\n"]},{"cell_type":"code","execution_count":null,"id":"2a2503e2","metadata":{"id":"2a2503e2","collapsed":true},"outputs":[],"source":["\n","import os, gc, torch\n","import numpy as np\n","from transformers import AutoModelForCausalLM, AutoProcessor\n","from PIL import Image\n","\n","# ---------------------------\n","# 0) Clean up prior models if cell re-run\n","# ---------------------------\n","if 'phi3_model' in globals():\n","    try:\n","        phi3_model.to('cpu')\n","    except Exception:\n","        pass\n","    try:\n","        del phi3_model\n","    except Exception:\n","        pass\n","gc.collect()\n","if torch.cuda.is_available():\n","    torch.cuda.empty_cache()\n","\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","\n","# ---------------------------\n","# 1) Load Phi-3 Vision (robust hybrid offload)\n","# ---------------------------\n","phi3_id   = \"microsoft/Phi-3-vision-128k-instruct\"\n","cuda_cap  = \"12GiB\"\n","off_dir   = \"offload_phi3\"\n","os.makedirs(off_dir, exist_ok=True)\n","\n","def _clear_cuda():\n","    gc.collect()\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","\n","# Try multiple key styles for max_memory to satisfy different HF/Accelerate versions\n","def _try_load_phi3_with_keys():\n","    gpu_idx = 0\n","    try:\n","        if torch.cuda.is_available():\n","            gpu_idx = torch.cuda.current_device()\n","    except Exception:\n","        pass\n","\n","    candidate_keys = [\n","        gpu_idx,                   # e.g., 0 (int)\n","        f\"cuda:{gpu_idx}\",         # e.g., \"cuda:0\"\n","        f\"{gpu_idx}\",              # e.g., \"0\"\n","        \"cuda\",                    # generic\n","    ]\n","\n","    last_err = None\n","    for key in candidate_keys:\n","        try:\n","            model = AutoModelForCausalLM.from_pretrained(\n","                phi3_id,\n","                trust_remote_code=True,\n","                torch_dtype=(torch.float16 if torch.cuda.is_available() else \"auto\"),\n","                device_map=\"auto\",\n","                max_memory={key: cuda_cap, \"cpu\": \"64GiB\"},\n","                offload_folder=off_dir,\n","                low_cpu_mem_usage=True,\n","                _attn_implementation=\"eager\",\n","            )\n","            return model\n","        except Exception as e:\n","            last_err = e\n","            _clear_cuda()\n","\n","\n","    try:\n","        model = AutoModelForCausalLM.from_pretrained(\n","            phi3_id,\n","            trust_remote_code=True,\n","            torch_dtype=(torch.float16 if torch.cuda.is_available() else \"auto\"),\n","            device_map=\"auto\",\n","            offload_folder=off_dir,\n","            low_cpu_mem_usage=True,\n","            _attn_implementation=\"eager\",\n","        )\n","        return model\n","    except Exception as e:\n","        raise last_err or e\n","\n","phi3_model = _try_load_phi3_with_keys()\n","phi3_model.eval()\n","\n","phi3_processor = AutoProcessor.from_pretrained(phi3_id, trust_remote_code=True)\n","if getattr(phi3_processor.tokenizer, \"pad_token_id\", None) is None:\n","    phi3_processor.tokenizer.pad_token = phi3_processor.tokenizer.eos_token\n","\n","blank_image = Image.new(\"RGB\", (224, 224), color=(255, 255, 255))\n","\n","# ---------------------------\n","# 2) Utilities\n","# ---------------------------\n","def _truncate_context(context: str, max_chars: int = 6000) -> str:\n","    return context[:max_chars]\n","\n","def _l2_normalize(v, eps=1e-12):\n","    v = np.asarray(v, dtype=np.float32)\n","    n = np.linalg.norm(v)\n","    return v / max(n, eps)\n","\n","\n","try:\n","    page_to_images\n","except NameError:\n","    from collections import defaultdict\n","    page_to_images = defaultdict(list)\n","    try:\n","        for im in images:\n","            page_to_images[im[\"page\"]].append(im[\"image\"])\n","    except Exception:\n","        pass\n","\n","# ---------------------------\n","# 3) Image selection\n","# ---------------------------\n","def select_images_for_query(query: str, top_k_images: int = 3, prefer_text_pages: bool = True):\n","    selected = []\n","\n","\n","    if prefer_text_pages:\n","        try:\n","            res = retrieve(query, top_k=5, where={\"type\": \"text\"})\n","            metas = (res or {}).get(\"metadatas\") or []\n","            if metas and isinstance(metas[0], list):\n","                metas = metas[0]\n","            pages = [m.get(\"page\") for m in metas if isinstance(m, dict) and \"page\" in m]\n","            for p in pages:\n","                for img in page_to_images.get(p, []):\n","                    if len(selected) < top_k_images:\n","                        selected.append(img)\n","        except Exception:\n","            pass\n","\n","\n","    if len(selected) < 1:\n","        try:\n","            q = encode_text([query])[0]\n","            q = _l2_normalize(q)\n","            res_img = collection.query(\n","                query_embeddings=[q.tolist()],\n","                n_results=top_k_images,\n","                include=[\"metadatas\"],\n","                where={\"type\": \"image\"}\n","            )\n","            metas = (res_img or {}).get(\"metadatas\") or []\n","            if metas and isinstance(metas[0], list):\n","                metas = metas[0]\n","            for m in metas:\n","                p = m.get(\"page\")\n","                if p in page_to_images:\n","                    for img in page_to_images[p]:\n","                        if len(selected) < top_k_images:\n","                            selected.append(img)\n","        except Exception:\n","            pass\n","\n","    return selected\n","\n","# ---------------------------\n","# 4) Core: generate_answer (Vision-only) with self-cleanup\n","# ---------------------------\n","def generate_answer(\n","    query: str,\n","    top_k: int = 3,\n","    max_tokens: int = 200,\n","):\n","    \"\"\"\n","    Vision-only Phi-3 answer generation with real PDF figures.\n","    Frees CUDA memory at the end of the call.\n","    \"\"\"\n","    import gc as _gc\n","    inputs = None\n","    output_ids = None\n","    imgs = None\n","    try:\n","\n","        hits = retrieve(query, top_k=top_k) or {}\n","        docs = hits.get(\"documents\") or []\n","\n","        # Flatten list-of-lists -> list[str]\n","        context_chunks = [d for lst in docs for d in (lst if isinstance(lst, list) else [lst]) if isinstance(d, str)]\n","        context = \"\\n\\n\".join(context_chunks) if context_chunks else \"No relevant context.\"\n","        context = _truncate_context(context)\n","\n","        # Select up to 3 relevant images\n","        imgs = select_images_for_query(query, top_k_images=3, prefer_text_pages=True)\n","        if not imgs:\n","            imgs = [blank_image]\n","\n","        # Build chat with matching <|image_i|> tags\n","        image_tags = \" \".join(f\"<|image_{i+1}|>\" for i in range(len(imgs)))\n","        messages = [{\n","            \"role\": \"user\",\n","            \"content\": f\"{image_tags}\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer based solely on the context.\"\n","        }]\n","        prompt = phi3_processor.tokenizer.apply_chat_template(\n","            messages, tokenize=False, add_generation_prompt=True\n","        )\n","\n","        # Tokenize + move to model device\n","        inputs = phi3_processor(text=prompt, images=imgs, return_tensors=\"pt\")\n","        device = next(phi3_model.parameters()).device\n","        inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","        # Generate (use_cache=False avoids DynamicCache issue)\n","        with torch.no_grad():\n","            output_ids = phi3_model.generate(\n","                **inputs,\n","                max_new_tokens=max_tokens,\n","                do_sample=False,\n","                temperature=0.0,\n","                eos_token_id=phi3_processor.tokenizer.eos_token_id,\n","                pad_token_id=phi3_processor.tokenizer.pad_token_id,\n","                use_cache=False,\n","            )\n","\n","        # Strip prompt and decode\n","        output_ids = output_ids[:, inputs[\"input_ids\"].shape[1]:]\n","        response = phi3_processor.batch_decode(\n","            output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n","        )[0].strip()\n","        return response\n","\n","    finally:\n","        # drop large tensors and clear cache each call\n","        try:\n","            del output_ids\n","        except Exception:\n","            pass\n","        try:\n","            if isinstance(inputs, dict):\n","                for k in list(inputs.keys()):\n","                    try: del inputs[k]\n","                    except Exception: pass\n","            del inputs\n","        except Exception:\n","            pass\n","        try:\n","            del imgs\n","        except Exception:\n","            pass\n","        _gc.collect()\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()\n","\n","# (Optional) quick sanity info\n","#try:\n","#    from pprint import pprint\n","#except Exception:\n","#    pass\n"]},{"cell_type":"code","execution_count":null,"id":"dX4ZZl7CN1up","metadata":{"id":"dX4ZZl7CN1up","collapsed":true},"outputs":[],"source":["# Minimal smoke test\n","answer = generate_answer(\n","    \"What is the main idea of the Transformer paper?\",\n","    top_k=1,\n","    max_tokens=60\n",")\n","print(answer)\n"]},{"cell_type":"code","execution_count":null,"id":"scoC9AjdZEUS","metadata":{"id":"scoC9AjdZEUS","collapsed":true},"outputs":[],"source":["answer = generate_answer(\n","    \"What is the purpose of positional encoding in the Transformer model?\",\n","    top_k=2,\n","    max_tokens=80\n",")\n","print(answer)\n"]},{"cell_type":"code","execution_count":null,"id":"mmvmjQ0ZMEg-","metadata":{"id":"mmvmjQ0ZMEg-"},"outputs":[],"source":["# This prompt requires a GPU with higher VRAM (an A100 works well).\n","answer = generate_answer(\n","    \"Describe the sinusoidal positional encoding figure and what the color bands mean.\",\n","    top_k=2,\n","    max_tokens=120\n",")\n","print(answer)\n"]},{"cell_type":"markdown","id":"6d8f07fa","metadata":{"id":"6d8f07fa"},"source":["## 6. Conclusion\n","\n","This notebook shows how to build a multimodal retrieval-augmented generation (RAG) pipeline step by step, without depending on high-level frameworks. The workflow includes parsing a PDF, embedding both text and figures using Jina CLIP, storing and retrieving vectors with ChromaDB, and finally generating grounded answers with Phi-3 Vision."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}
