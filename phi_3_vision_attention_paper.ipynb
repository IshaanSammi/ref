{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3295b0a3",
      "metadata": {
        "id": "3295b0a3"
      },
      "source": [
        "## 1. Document ingestion\n",
        "\n",
        "We first download the paper and break it into manageable units. The code below fetches the PDF using `requests` and uses `fitz` to iterate through pages. Each page’s text is split into fixed-size chunks so that it can be embedded. Images are also extracted and stored for optional multimodal retrieval.\n",
        "\n",
        "> **Note**: If downloading fails due to network restrictions, manually place the PDF in the working directory and set `pdf_path` accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "035c515a",
      "metadata": {
        "id": "035c515a"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import io\n",
        "import requests\n",
        "import fitz\n",
        "from PIL import Image\n",
        "\n",
        "# Download the Transformer paper\n",
        "pdf_url = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
        "pdf_path = \"attention.pdf\"\n",
        "\n",
        "# Fetch the paper only if it isn’t already present\n",
        "if not os.path.exists(pdf_path):\n",
        "    response = requests.get(pdf_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "    if response.status_code == 200:\n",
        "        with open(pdf_path, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "    else:\n",
        "        raise RuntimeError(f\"Failed to download PDF (status {response.status_code}). Please download it manually and place it at {pdf_path}.\")\n",
        "\n",
        "# Parse the PDF\n",
        "doc = fitz.open(pdf_path)\n",
        "chunks = []  # list of {page, text}\n",
        "images = []  # list of {page, image}\n",
        "chunk_size = 500  # characters per chunk\n",
        "for page_num in range(doc.page_count):\n",
        "    page = doc[page_num]\n",
        "    text = page.get_text().strip()\n",
        "    # Create chunks of roughly `chunk_size` characters\n",
        "    for i in range(0, len(text), chunk_size):\n",
        "        chunk_text = text[i:i + chunk_size]\n",
        "        chunks.append({\"page\": page_num, \"text\": chunk_text})\n",
        "    # Extract images from the page\n",
        "    for img_index, img_info in enumerate(page.get_images(full=True)):\n",
        "        xref = img_info[0]\n",
        "        base_image = doc.extract_image(xref)\n",
        "        image_bytes = base_image[\"image\"]\n",
        "        pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "        images.append({\"page\": page_num, \"image\": pil_image})\n",
        "\n",
        "doc.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c59fe34a",
      "metadata": {
        "id": "c59fe34a"
      },
      "source": [
        "## 2. Embedding with Jina CLIP V1\n",
        "\n",
        "[Jina CLIP V1](https://huggingface.co/jinaai/jina-clip-v1) is a multimodal embedding model capable of mapping both text and images into the same vector space. Loading the model with the Hugging Face `transformers` library is straightforward:\n",
        "\n",
        "\n",
        "\n",
        "The model’s remote code adds `encode_text` and `encode_image` methods which return tensors. We wrap these calls in helper functions so the rest of the pipeline remains agnostic to the underlying library.【208295142115717†L92-L121】\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D_q_fWZqB8_C",
      "metadata": {
        "id": "D_q_fWZqB8_C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModel\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---- Load Jina CLIP ----\n",
        "clip_model = AutoModel.from_pretrained(\"jinaai/jina-clip-v1\", trust_remote_code=True)\n",
        "clip_model.eval()\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "USE_FP16 = False  # flip to True only if GPU hits memory limits\n",
        "\n",
        "if USE_CUDA:\n",
        "    clip_model = clip_model.to(\"cuda\")\n",
        "    if USE_FP16:\n",
        "        clip_model = clip_model.half()\n",
        "\n",
        "# Build inputs from Cell 1 outputs\n",
        "texts = [c[\"text\"] for c in chunks]  # from Cell 1\n",
        "image_list = [im[\"image\"] for im in images] if images else []\n",
        "\n",
        "# ---- Build page→images map for later use (Vision model) ----\n",
        "page_to_images = defaultdict(list)\n",
        "for im in images:\n",
        "    page_to_images[im[\"page\"]].append(im[\"image\"])\n",
        "\n",
        "# ---- Simple batching helper ----\n",
        "def batched(seq, n):\n",
        "    for i in range(0, len(seq), n):\n",
        "        yield seq[i:i+n]\n",
        "\n",
        "# ---- Encode helpers (return numpy arrays) ----\n",
        "@torch.no_grad()\n",
        "def encode_text(text_list):\n",
        "    # Jina-CLIP remote code returns numpy arrays\n",
        "    return clip_model.encode_text(text_list)\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_images(pil_images):\n",
        "    return clip_model.encode_image(pil_images)\n",
        "\n",
        "# ---- Run encoders in batches ----\n",
        "TEXT_BATCH = 64\n",
        "IMG_BATCH = 16\n",
        "\n",
        "text_embeds = []\n",
        "for b in batched(texts, TEXT_BATCH):\n",
        "    if b:  # skip empties\n",
        "        text_embeds.append(encode_text(b))\n",
        "text_embeddings = np.vstack(text_embeds) if text_embeds else np.zeros((0, 768), dtype=np.float32)\n",
        "\n",
        "image_embeds = []\n",
        "for b in batched(image_list, IMG_BATCH):\n",
        "    if b:\n",
        "        image_embeds.append(encode_images(b))\n",
        "image_embeddings = np.vstack(image_embeds) if image_embeds else np.zeros((0, 768), dtype=np.float32)\n",
        "\n",
        "# ---- L2-normalize for cosine similarity ----\n",
        "def l2_normalize(a, eps=1e-12):\n",
        "    if a.size == 0:\n",
        "        return a\n",
        "    norms = np.linalg.norm(a, axis=1, keepdims=True)\n",
        "    return a / np.maximum(norms, eps)\n",
        "\n",
        "text_embeddings = l2_normalize(text_embeddings)\n",
        "image_embeddings = l2_normalize(image_embeddings)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "257cab70",
      "metadata": {
        "id": "257cab70"
      },
      "source": [
        "## 3. Storing embeddings in Chroma\n",
        "\n",
        "The [Chroma](https://docs.trychroma.com/) vector database is used to persist our embeddings. It performs approximate nearest-neighbour search to quickly retrieve the most relevant chunks. After creating a collection, we insert the text and image embeddings along with metadata. Each entry uses a unique ID so we can trace results back to the original page or chunk.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05fe6aee",
      "metadata": {
        "id": "05fe6aee"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from math import ceil\n",
        "\n",
        "\n",
        "db_client = chromadb.Client()\n",
        "\n",
        "collection = db_client.get_or_create_collection(name=\"transformer_paper\")\n",
        "\n",
        "# --- Prepare inputs ---\n",
        "texts = [c[\"text\"] for c in chunks]\n",
        "text_ids = [f\"text_{i}\" for i in range(len(texts))]\n",
        "text_metadatas = [{\"page\": c[\"page\"], \"type\": \"text\", \"idx\": i} for i, c in enumerate(chunks)]\n",
        "\n",
        "\n",
        "try:\n",
        "    _ = text_embeddings\n",
        "except NameError:\n",
        "    text_embeddings = encode_text(texts)\n",
        "\n",
        "# Convert to list-of-lists for Chroma\n",
        "text_vectors = [row.tolist() for row in text_embeddings]\n",
        "\n",
        "# --- Add in batches to avoid payload issues ---\n",
        "BATCH = 512\n",
        "for b in range(ceil(len(texts) / BATCH)):\n",
        "    s = b * BATCH\n",
        "    e = min((b + 1) * BATCH, len(texts))\n",
        "    if s < e:\n",
        "        collection.add(\n",
        "            ids=text_ids[s:e],\n",
        "            embeddings=text_vectors[s:e],\n",
        "            metadatas=text_metadatas[s:e],\n",
        "            documents=texts[s:e],\n",
        "        )\n",
        "\n",
        "if images:\n",
        "    pil_images = [im[\"image\"] for im in images]\n",
        "    image_ids = [f\"img_{i}\" for i in range(len(images))]\n",
        "    image_metadatas = [{\"page\": im[\"page\"], \"type\": \"image\", \"idx\": i} for i, im in enumerate(images)]\n",
        "\n",
        "\n",
        "    try:\n",
        "        _ = image_embeddings\n",
        "    except NameError:\n",
        "        image_embeddings = encode_images(pil_images)\n",
        "\n",
        "    image_vectors = [row.tolist() for row in image_embeddings]\n",
        "\n",
        "    for b in range(ceil(len(pil_images) / BATCH)):\n",
        "        s = b * BATCH\n",
        "        e = min((b + 1) * BATCH, len(pil_images))\n",
        "        if s < e:\n",
        "            collection.add(\n",
        "                ids=image_ids[s:e],\n",
        "                embeddings=image_vectors[s:e],\n",
        "                metadatas=image_metadatas[s:e],\n",
        "                documents=[\"\"] * (e - s),  # no text docs for images\n",
        "            )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ef256a2",
      "metadata": {
        "id": "4ef256a2"
      },
      "source": [
        "## 4. Retrieval\n",
        "\n",
        "Retrieval is handled by querying the Chroma collection with a new embedding. Given a query string, we compute its embedding using Jina CLIP and request the top-`k` nearest neighbours. The function below returns the matching documents, metadata and distances.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de693fed",
      "metadata": {
        "id": "de693fed"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def _l2_normalize(v, eps=1e-12):\n",
        "    if v.ndim == 1:\n",
        "        n = np.linalg.norm(v) or eps\n",
        "        return v / max(n, eps)\n",
        "    n = np.linalg.norm(v, axis=1, keepdims=True)\n",
        "    return v / np.maximum(n, eps)\n",
        "\n",
        "def retrieve(query, top_k=3, where=None):\n",
        "    \"\"\"\n",
        "    Retrieve most relevant text chunks for a query.\n",
        "    - Normalizes query embedding to match index normalization.\n",
        "    - Optional `where` filter; defaults to text-only.\n",
        "    \"\"\"\n",
        "    if where is None:\n",
        "        where = {\"type\": \"text\"}\n",
        "\n",
        "    # Encode + normalize query\n",
        "    q_emb = encode_text([query])[0]  # (768,)\n",
        "    q_emb = _l2_normalize(q_emb)\n",
        "\n",
        "    # Bound top_k by collection size\n",
        "    count = collection.count()\n",
        "    k = min(top_k, max(count, 0))\n",
        "\n",
        "    if k == 0:\n",
        "        return {\"documents\": [], \"metadatas\": [], \"distances\": []}\n",
        "\n",
        "    res = collection.query(\n",
        "        query_embeddings=[q_emb.tolist()],\n",
        "        n_results=k,\n",
        "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
        "        where=where\n",
        "    )\n",
        "    return res\n",
        "\n",
        "def show_results(res, max_chars=160):\n",
        "    \"\"\"Pretty-print results for quick sanity checks.\"\"\"\n",
        "    if not res or not res.get(\"documents\"):\n",
        "        return\n",
        "    docs = res[\"documents\"][0]\n",
        "    metas = res[\"metadatas\"][0]\n",
        "    dists = res[\"distances\"][0]\n",
        "    for i, (doc, meta, dist) in enumerate(zip(docs, metas, dists), 1):\n",
        "        snippet = (doc[:max_chars] + \"…\") if doc and len(doc) > max_chars else (doc or \"\")\n",
        "        page = meta.get(\"page\", \"?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35c5d336",
      "metadata": {
        "id": "35c5d336"
      },
      "source": [
        "## 5. Generation with Phi-3 Vision\n",
        "\n",
        "The [Phi-3 Vision](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct) model is a multimodal generative model capable of processing a single image plus a long text prompt. It expects input in a special chat format:\n",
        "\n",
        "```\n",
        "<|user|>\n",
        "<|image_1|>\n",
        "{prompt}<|end|>\n",
        "<|assistant|>\n",
        "\n",
        "```\n",
        "\n",
        "An optional multi-turn format can also be used, but for our retrieval augmented generation we only need a single turn. When loading the model with `transformers`, be sure to pass `trust_remote_code=True` and disable flash attention on CPU by setting `_attn_implementation=\"eager\"`. A small blank image is supplied if no relevant image was retrieved.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a2503e2",
      "metadata": {
        "id": "2a2503e2"
      },
      "outputs": [],
      "source": [
        "# === Phi-3 Vision: robust hybrid offload + real-image selection + generation (memory-hygiene) ===\n",
        "import os, gc, torch\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "from PIL import Image\n",
        "\n",
        "# ---------------------------\n",
        "# 0) Clean up prior models if cell re-run\n",
        "# ---------------------------\n",
        "if 'phi3_model' in globals():\n",
        "    try:\n",
        "        phi3_model.to('cpu')\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        del phi3_model\n",
        "    except Exception:\n",
        "        pass\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Load Phi-3 Vision (robust hybrid offload)\n",
        "# ---------------------------\n",
        "phi3_id   = \"microsoft/Phi-3-vision-128k-instruct\"\n",
        "cuda_cap  = \"12GiB\"\n",
        "off_dir   = \"offload_phi3\"\n",
        "os.makedirs(off_dir, exist_ok=True)\n",
        "\n",
        "def _clear_cuda():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# Try multiple key styles for max_memory to satisfy different HF/Accelerate versions\n",
        "def _try_load_phi3_with_keys():\n",
        "    gpu_idx = 0\n",
        "    try:\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_idx = torch.cuda.current_device()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    candidate_keys = [\n",
        "        gpu_idx,                   # e.g., 0 (int)\n",
        "        f\"cuda:{gpu_idx}\",         # e.g., \"cuda:0\"\n",
        "        f\"{gpu_idx}\",              # e.g., \"0\"\n",
        "        \"cuda\",                    # generic\n",
        "    ]\n",
        "\n",
        "    last_err = None\n",
        "    for key in candidate_keys:\n",
        "        try:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                phi3_id,\n",
        "                trust_remote_code=True,\n",
        "                torch_dtype=(torch.float16 if torch.cuda.is_available() else \"auto\"),\n",
        "                device_map=\"auto\",\n",
        "                max_memory={key: cuda_cap, \"cpu\": \"64GiB\"},\n",
        "                offload_folder=off_dir,\n",
        "                low_cpu_mem_usage=True,\n",
        "                _attn_implementation=\"eager\",\n",
        "            )\n",
        "            return model\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            _clear_cuda()  # free any partial allocations before next attempt\n",
        "\n",
        "    # Fallback: no max_memory hint\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            phi3_id,\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=(torch.float16 if torch.cuda.is_available() else \"auto\"),\n",
        "            device_map=\"auto\",\n",
        "            offload_folder=off_dir,\n",
        "            low_cpu_mem_usage=True,\n",
        "            _attn_implementation=\"eager\",\n",
        "        )\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        raise last_err or e\n",
        "\n",
        "phi3_model = _try_load_phi3_with_keys()\n",
        "phi3_model.eval()\n",
        "\n",
        "phi3_processor = AutoProcessor.from_pretrained(phi3_id, trust_remote_code=True)\n",
        "if getattr(phi3_processor.tokenizer, \"pad_token_id\", None) is None:\n",
        "    phi3_processor.tokenizer.pad_token = phi3_processor.tokenizer.eos_token\n",
        "\n",
        "blank_image = Image.new(\"RGB\", (224, 224), color=(255, 255, 255))\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Utilities\n",
        "# ---------------------------\n",
        "def _truncate_context(context: str, max_chars: int = 6000) -> str:\n",
        "    return context[:max_chars]\n",
        "\n",
        "def _l2_normalize(v, eps=1e-12):\n",
        "    v = np.asarray(v, dtype=np.float32)\n",
        "    n = np.linalg.norm(v)\n",
        "    return v / max(n, eps)\n",
        "\n",
        "# Ensure page->images map exists\n",
        "try:\n",
        "    page_to_images\n",
        "except NameError:\n",
        "    from collections import defaultdict\n",
        "    page_to_images = defaultdict(list)\n",
        "    try:\n",
        "        for im in images:\n",
        "            page_to_images[im[\"page\"]].append(im[\"image\"])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Image selection\n",
        "# ---------------------------\n",
        "def select_images_for_query(query: str, top_k_images: int = 3, prefer_text_pages: bool = True):\n",
        "    selected = []\n",
        "\n",
        "    # Prefer images from pages of top text hits\n",
        "    if prefer_text_pages:\n",
        "        try:\n",
        "            res = retrieve(query, top_k=5, where={\"type\": \"text\"})\n",
        "            metas = (res or {}).get(\"metadatas\") or []\n",
        "            if metas and isinstance(metas[0], list):\n",
        "                metas = metas[0]\n",
        "            pages = [m.get(\"page\") for m in metas if isinstance(m, dict) and \"page\" in m]\n",
        "            for p in pages:\n",
        "                for img in page_to_images.get(p, []):\n",
        "                    if len(selected) < top_k_images:\n",
        "                        selected.append(img)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Fallback: cross-modal query into Chroma's image vectors\n",
        "    if len(selected) < 1:\n",
        "        try:\n",
        "            q = encode_text([query])[0]\n",
        "            q = _l2_normalize(q)\n",
        "            res_img = collection.query(\n",
        "                query_embeddings=[q.tolist()],\n",
        "                n_results=top_k_images,\n",
        "                include=[\"metadatas\"],\n",
        "                where={\"type\": \"image\"}\n",
        "            )\n",
        "            metas = (res_img or {}).get(\"metadatas\") or []\n",
        "            if metas and isinstance(metas[0], list):\n",
        "                metas = metas[0]\n",
        "            for m in metas:\n",
        "                p = m.get(\"page\")\n",
        "                if p in page_to_images:\n",
        "                    for img in page_to_images[p]:\n",
        "                        if len(selected) < top_k_images:\n",
        "                            selected.append(img)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return selected\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Core: generate_answer (Vision-only) with self-cleanup\n",
        "# ---------------------------\n",
        "def generate_answer(\n",
        "    query: str,\n",
        "    top_k: int = 3,\n",
        "    max_tokens: int = 200,\n",
        "):\n",
        "    \"\"\"\n",
        "    Vision-only Phi-3 answer generation with real PDF figures.\n",
        "    Frees CUDA memory at the end of the call.\n",
        "    \"\"\"\n",
        "    import gc as _gc\n",
        "    inputs = None\n",
        "    output_ids = None\n",
        "    imgs = None\n",
        "    try:\n",
        "        # Retrieve supporting chunks\n",
        "        hits = retrieve(query, top_k=top_k) or {}\n",
        "        docs = hits.get(\"documents\") or []\n",
        "\n",
        "        # Flatten list-of-lists -> list[str]\n",
        "        context_chunks = [d for lst in docs for d in (lst if isinstance(lst, list) else [lst]) if isinstance(d, str)]\n",
        "        context = \"\\n\\n\".join(context_chunks) if context_chunks else \"No relevant context.\"\n",
        "        context = _truncate_context(context)\n",
        "\n",
        "        # Select up to 3 relevant images\n",
        "        imgs = select_images_for_query(query, top_k_images=3, prefer_text_pages=True)\n",
        "        if not imgs:\n",
        "            imgs = [blank_image]\n",
        "\n",
        "        # Build chat with matching <|image_i|> tags\n",
        "        image_tags = \" \".join(f\"<|image_{i+1}|>\" for i in range(len(imgs)))\n",
        "        messages = [{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"{image_tags}\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer based solely on the context.\"\n",
        "        }]\n",
        "        prompt = phi3_processor.tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        # Tokenize + move to model device\n",
        "        inputs = phi3_processor(text=prompt, images=imgs, return_tensors=\"pt\")\n",
        "        device = next(phi3_model.parameters()).device\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Generate (use_cache=False avoids DynamicCache issue)\n",
        "        with torch.no_grad():\n",
        "            output_ids = phi3_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_tokens,\n",
        "                do_sample=False,\n",
        "                temperature=0.0,\n",
        "                eos_token_id=phi3_processor.tokenizer.eos_token_id,\n",
        "                pad_token_id=phi3_processor.tokenizer.pad_token_id,\n",
        "                use_cache=False,\n",
        "            )\n",
        "\n",
        "        # Strip prompt and decode\n",
        "        output_ids = output_ids[:, inputs[\"input_ids\"].shape[1]:]\n",
        "        response = phi3_processor.batch_decode(\n",
        "            output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "        )[0].strip()\n",
        "        return response\n",
        "\n",
        "    finally:\n",
        "        # drop large tensors and clear cache each call\n",
        "        try:\n",
        "            del output_ids\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            if isinstance(inputs, dict):\n",
        "                for k in list(inputs.keys()):\n",
        "                    try: del inputs[k]\n",
        "                    except Exception: pass\n",
        "            del inputs\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            del imgs\n",
        "        except Exception:\n",
        "            pass\n",
        "        _gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "# (Optional) quick sanity info\n",
        "#try:\n",
        "#    from pprint import pprint\n",
        "#except Exception:\n",
        "#    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dX4ZZl7CN1up",
      "metadata": {
        "id": "dX4ZZl7CN1up"
      },
      "outputs": [],
      "source": [
        "# Minimal smoke test\n",
        "answer = generate_answer(\n",
        "    \"What is the main idea of the Transformer paper?\",\n",
        "    top_k=1,\n",
        "    max_tokens=60\n",
        ")\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scoC9AjdZEUS",
      "metadata": {
        "id": "scoC9AjdZEUS"
      },
      "outputs": [],
      "source": [
        "answer = generate_answer(\n",
        "    \"What is the purpose of positional encoding in the Transformer model?\",\n",
        "    top_k=2,\n",
        "    max_tokens=80\n",
        ")\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mmvmjQ0ZMEg-",
      "metadata": {
        "id": "mmvmjQ0ZMEg-"
      },
      "outputs": [],
      "source": [
        "# This prompt requires a GPU with higher VRAM (an A100 works well).\n",
        "answer = generate_answer(\n",
        "    \"Describe the sinusoidal positional encoding figure and what the color bands mean.\",\n",
        "    top_k=2,\n",
        "    max_tokens=120\n",
        ")\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d8f07fa",
      "metadata": {
        "id": "6d8f07fa"
      },
      "source": [
        "## 6. Conclusion\n",
        "\n",
        "This notebook demonstrates how to build a multimodal retrieval-augmented generation (RAG) pipeline from the ground up without relying on high-level frameworks. It parses a PDF, embeds text and figures using Jina CLIP, stores and retrieves vectors through ChromaDB, and finally uses Phi-3 Vision to generate grounded answers.\n",
        "The notebook employs hybrid offloading and memory-efficient loading to handle large models on limited hardware. It runs smoothly on a T4 GPU for smaller prompts, but for complex or larger inputs, an A100 GPU is recommended. If you encounter resource or download issues, try pre-installing dependencies and ensuring adequate GPU or CPU memory."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
